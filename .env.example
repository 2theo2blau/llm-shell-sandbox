# LLM Configuration
OLLAMA_API_URL=http://host.docker.internal:11434/api/chat
OLLAMA_MODEL_NAME=mistral-nemo:12b-instruct-2407-fp16
OLLAMA_TEMPERATURE=0.3
OLLAMA_CONTEXT_LENGTH=8192

# Application Configuration
MAX_COMMANDS=10
TIMEOUT_SECONDS=120